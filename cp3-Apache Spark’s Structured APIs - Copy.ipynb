{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/databricks/LearningSparkV2\n",
    "\n",
    "\n",
    "https://github.com/RodrigoLima82/spark-certification\n",
    "\n",
    "reference repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-notations\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc= SparkContext()\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark: What’s Underneath an RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|    _1|  _2|\n",
      "+------+----+\n",
      "|Brooke|22.5|\n",
      "| Denny|31.0|\n",
      "|    TD|35.0|\n",
      "| Jules|30.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In Python\n",
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)])\n",
    "# Use map and reduceByKey transformations with their lambda\n",
    "# expressions to aggregate and then compute average\n",
    "agesRDD = (dataRDD\n",
    ".map(lambda x: (x[0], (x[1], 1)))\n",
    ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    ".map(lambda x: (x[0], x[1][0]/x[1][1])))\n",
    "\n",
    "dfnew = spark.createDataFrame(agesRDD)\n",
    "dfnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    "StructField(\"title\", StringType(), False),\n",
    "StructField(\"pages\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "[2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    "[3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "[4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "[\"twitter\", \"FB\"]],\n",
    "[5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "[6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    # Show the DataFrame; it should reflect our table above\n",
    "    blogs_df.show()\n",
    "\n",
    "    # Print the schema used by Spark to process the DataFrame\n",
    "    print(blogs_df.printSchema())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blogs_df.select(col(\"Id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(col(\"AuthorsId\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+-----------------+---------+-----+-------------+-----------+\n",
      "| Id|    First|Last|              Url|Published| Hits|    Campaigns|Big_Hitters|\n",
      "+---+---------+----+-----------------+---------+-----+-------------+-----------+\n",
      "|  4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568|[twitter, FB]|       true|\n",
      "+---+---------+----+-----------------+---------+-----+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with filter\n",
    "df2 = blogs_df.withColumn(\"Big_Hitters\", (expr(\"Hits > 10000\")))\n",
    "df2.filter((df2.Big_Hitters ==\"true\") & (df2.Id == 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These statements return the same value, showing that\n",
    "# expr is the same as a col method call\n",
    "blogs_df.select(expr(\"Hits\")).show(2)\n",
    "blogs_df.select(col(\"Hits\")).show(2)\n",
    "blogs_df.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by column \"Id\" in descending order\n",
    "blogs_df.sort(col(\"Id\").desc()).show()\n",
    "#blogsDF.sort($\"Id\".desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    "[\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[1]\n",
    "'Reynold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "StructField('UnitID', StringType(), True),\n",
    "StructField('IncidentNumber', IntegerType(), True),\n",
    "StructField('CallType', StringType(), True),\n",
    "StructField('CallDate', StringType(), True),\n",
    "StructField('WatchDate', StringType(), True),\n",
    "StructField('CallFinalDisposition', StringType(), True),\n",
    "StructField('AvailableDtTm', StringType(), True),\n",
    "StructField('Address', StringType(), True),\n",
    "StructField('City', StringType(), True),\n",
    "StructField('Zipcode', IntegerType(), True),\n",
    "StructField('Battalion', StringType(), True),\n",
    "StructField('StationArea', StringType(), True),\n",
    "StructField('Box', StringType(), True),\n",
    "StructField('OriginalPriority', StringType(), True),\n",
    "StructField('Priority', StringType(), True),\n",
    "StructField('FinalPriority', IntegerType(), True),\n",
    "StructField('ALSUnit', BooleanType(), True),\n",
    "StructField('CallTypeGroup', StringType(), True),\n",
    "StructField('NumAlarms', IntegerType(), True),\n",
    "StructField('UnitType', StringType(), True),\n",
    "StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "StructField('FirePreventionDistrict', StringType(), True),\n",
    "StructField('SupervisorDistrict', StringType(), True),\n",
    "StructField('Neighborhood', StringType(), True),\n",
    "StructField('Location', StringType(), True),\n",
    "StructField('RowID', StringType(), True),\n",
    "StructField('Delay', FloatType(), True)])\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"C:/Lenzi/Spark/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "#fire_df=fire_df.na.drop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175296"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"C:/Lenzi/Spark/spark-certification-main/files/parquet/\"\n",
    "fire_df.write.format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = 'fire_table8' # name of the table\n",
    "fire_df.write.format(\"parquet\").saveAsTable(parquet_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In Python\n",
    "few_fire_df = (fire_df\n",
    ".select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    ".where(col(\"CallType\") != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|           168571|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, return number of distinct types of calls using countDistinct()\n",
    "from pyspark.sql.functions import *\n",
    "(fire_df\n",
    ".select(\"IncidentNumber\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".agg(countDistinct(\"IncidentNumber\").alias(\"DistinctCallTypes\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|CallType                     |\n",
      "+-----------------------------+\n",
      "|Elevator / Escalator Rescue  |\n",
      "|Marine Fire                  |\n",
      "|Aircraft Emergency           |\n",
      "|Administrative               |\n",
      "|Alarms                       |\n",
      "|Odor (Strange / Unknown)     |\n",
      "|Citizen Assist / Service Call|\n",
      "|HazMat                       |\n",
      "|Watercraft in Distress       |\n",
      "|Explosion                    |\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In Python, filter for only distinct non-null CallTypes from all the rows\n",
    "(fire_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".distinct()\n",
    ".show(10, False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming, adding, and dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# no livro os autores mencionam que adicionaram uma coluna Delay mas não mencionam o que fizeram\n",
    "#adicionamos aqui um numero aleatório apenas para testes\n",
    "new_fire_df = fire_df.withColumn(\"Delay\",when(rand() > 0.5, 1).otherwise(4)).withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|4                    |\n",
      "|4                    |\n",
      "|4                    |\n",
      "|4                    |\n",
      "|4                    |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\") > 1)\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|AvgResponseDelayedinMins|\n",
      "+------------------------+\n",
      "|      2.5056989320920042|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test query - avg response\n",
    "new_fire_df = fire_df.withColumn(\"Delay\",when(rand() > 0.5, 1).otherwise(4)).withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "\n",
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".agg(avg(\"ResponseDelayedinMins\").alias(\"AvgResponseDelayedinMins\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFunc(df, rows):\n",
    "    df = df.pandas_api()\n",
    "    return df.head(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fire_df.createOrReplaceTempView(\"new_fire_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.sql('''select * from new_fire_vw''')\n",
    "#df2 = df.toPandas()\n",
    "\n",
    "#displayFunc(df2,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "fire_ts_df = (new_fire_df\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------------------+\n",
      "|CallDate  |WatchDate |AvailableDtTm         |\n",
      "+----------+----------+----------------------+\n",
      "|01/11/2002|01/10/2002|01/11/2002 01:51:44 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 03:01:18 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 02:39:50 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 04:16:46 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 06:01:58 AM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 08:03:26 AM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 09:46:44 AM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 09:58:53 AM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 12:06:57 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 01:08:40 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 03:31:02 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 02:59:04 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:22:49 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:18:33 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:09:08 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:09:08 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:09:08 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:34:23 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:51:31 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 04:51:12 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 05:17:15 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 05:46:30 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 06:48:01 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 09:03:17 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 10:08:48 PM|\n",
      "|01/11/2002|01/11/2002|01/11/2002 10:56:59 PM|\n",
      "|01/12/2002|01/11/2002|01/12/2002 02:04:06 AM|\n",
      "|01/12/2002|01/11/2002|01/12/2002 01:56:32 AM|\n",
      "|01/12/2002|01/11/2002|01/12/2002 04:17:22 AM|\n",
      "|01/12/2002|01/11/2002|01/12/2002 04:23:31 AM|\n",
      "|01/12/2002|01/11/2002|01/12/2002 06:27:31 AM|\n",
      "|01/12/2002|01/11/2002|null                  |\n",
      "|01/12/2002|01/12/2002|01/12/2002 11:07:36 AM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 11:28:40 AM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 12:15:25 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 01:23:04 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 01:05:52 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 02:20:25 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 01:03:10 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 01:22:58 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 04:25:35 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 04:46:59 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 07:15:23 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 07:26:52 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 07:54:42 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 09:14:35 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 08:44:01 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 09:14:13 PM|\n",
      "|01/12/2002|01/12/2002|01/12/2002 11:19:49 PM|\n",
      "|01/13/2002|01/12/2002|01/13/2002 01:05:26 AM|\n",
      "+----------+----------+----------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(new_fire_df\n",
    ".select(\"CallDate\", \"WatchDate\", \"AvailableDtTm\")\n",
    ".show(50, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 08:03:26|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 09:46:44|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 09:58:53|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 12:06:57|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 13:08:40|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 15:31:02|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 14:59:04|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:22:49|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:18:33|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:34:23|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:51:31|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:51:12|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 17:17:15|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 17:46:30|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 18:48:01|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 21:03:17|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 22:08:48|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 22:56:59|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|2002-01-12 02:04:06|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|2002-01-12 01:56:32|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|2002-01-12 04:17:22|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|2002-01-12 04:23:31|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|2002-01-12 06:27:31|\n",
      "|2002-01-12 00:00:00|2002-01-11 00:00:00|null               |\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 11:07:36|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 11:28:40|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 12:15:25|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 13:23:04|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 13:05:52|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 14:20:25|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 13:03:10|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 13:22:58|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 16:25:35|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 16:46:59|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 19:15:23|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 19:26:52|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 19:54:42|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 21:14:35|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 20:44:01|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 21:14:13|\n",
      "|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 23:19:49|\n",
      "|2002-01-13 00:00:00|2002-01-12 00:00:00|2002-01-13 01:05:26|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Select the converted columns\n",
    "(fire_ts_df\n",
    ".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    ".show(50, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "(fire_ts_df\n",
    ".select(year('IncidentDate'))\n",
    ".distinct()\n",
    ".orderBy(year('IncidentDate'))\n",
    ".show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "(fire_ts_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show(n=10, truncate=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other common DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|        2.5056989320920042|                         1|                         4|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "import pyspark.sql.functions as F\n",
    "(fire_ts_df\n",
    ".select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    ".show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames Versus Datasets\n",
    "By now you may be wondering why and when you should use DataFrames or Datasets.\n",
    "In many cases either will do, depending on the languages you are working in, but\n",
    "there are some situations where one is preferable to the other. Here are a few\n",
    "examples:\n",
    "\n",
    " • If you want to tell Spark what to do, not how to do it, use DataFrames or Datasets.\n",
    "\n",
    " • If you want rich semantics, high-level abstractions, and DSL operators, use Data‐Frames or Datasets.\n",
    "\n",
    " • If you want strict compile-time type safety and don’t mind creating multiple case classes for a specific Dataset[T], use Datasets.\n",
    "\n",
    " • If your processing demands high-level expressions, filters, maps, aggregations,computing averages or sums, SQL queries, columnar access, or use of relational operators on semi-structured data, use DataFrames or Datasets.\n",
    "\n",
    " • If your processing dictates relational transformations similar to SQL-like queries,use DataFrames.\n",
    " \n",
    " • If you want to take advantage of and benefit from Tungsten’s efficient serializationwith Encoders, use Datasets.\n",
    "\n",
    " • If you want unification, code optimization, and simplification of APIs across Spark components, use DataFrames.\n",
    "\n",
    " • If you are an R user, use DataFrames.\n",
    "\n",
    " • If you are a Python user, use DataFrames and drop down to RDDs if you need more control.\n",
    " \n",
    " • If you want space and speed efficiency, use DataFrames."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use RDDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Are using a third-party package that’s written using RDDs\n",
    "\n",
    "• Can forgo the code optimization, efficient space utilization, and performance benefits available with DataFrames and Datasets\n",
    "\n",
    "• Want to precisely instruct Spark how to do a query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through each of the four query optimization phases..\n",
    "\n",
    "Phase 1: Analysis\n",
    "The Spark SQL engine begins by generating an abstract syntax tree (AST) for the SQL or DataFrame query. In this initial phase, any columns or table names will be resolved by consulting an internal Catalog, a programmatic interface to Spark SQL that holds a list of names of columns, data types, functions, tables, databases, etc. Once they’ve all been successfully resolved, the query proceeds to the next phase.\n",
    "\n",
    "Phase\n",
    " 2: Logical optimization\n",
    "As Figure 3-4 shows, this phase comprises two internal stages. Applying a standardrule based optimization approach, the Catalyst optimizer will first construct a set of multiple plans and then, using its cost-based optimizer (CBO), assign costs to each plan. These plans are laid out as operator trees (like in Figure 3-5); they may include,for example, the process of constant folding, predicate pushdown, projection pruning,Boolean expression simplification, etc. This logical plan is the input into thephysical plan.\n",
    "\n",
    "Phase 3: Physical planning\n",
    "In this phase, Spark SQL generates an optimal physical plan for the selected logical plan, using physical operators that match those available in the Spark execution engine.\n",
    "\n",
    "Phase 4: Code generation\n",
    "The final phase of query optimization involves generating efficient Java bytecode to run on each machine. Because Spark SQL can operate on data sets loaded in memory, Spark can use state-of-the-art compiler technology for code generation to speed up execution. In other words, it acts as a compiler. Project Tungsten, which facilitates\n",
    "whole-stage code generation, plays a role here.\n",
    "Just what is whole-stage code generation? It’s a physical query optimization phase that collapses the whole query into a single function, getting rid of virtual function calls and employing CPU registers for intermediate data. The second-generation Tungsten\n",
    "engine, introduced in Spark 2.0, uses this approach to generate compact RDD code for final execution. This streamlined strategy significantly improves CPU efficiency and performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ed1cb665153056a227012879eeccb1f3bdc4741eab748c2bfd24c235a4ad9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
