{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/databricks/LearningSparkV2\n",
    "\n",
    "\n",
    "https://github.com/RodrigoLima82/spark-certification\n",
    "\n",
    "reference repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-notations\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport()      \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc= SparkContext()\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL in Spark Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"C:/Lenzi/Spark/spark developer preparation/data/flights/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"C:/Lenzi/Spark/spark developer preparation/data/flights/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# defining schema option\n",
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"Schema\", \"schema\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select * from us_delay_flights_tbl limit 10''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "|01161210|  225|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#voo mais longo\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    ".where(col(\"distance\") > 1000)\n",
    ".orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "|01161210|  225|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "|01161210|  225|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"date\",\"delay\", \"origin\", \"destination\")\n",
    ".where(\"delay > 120 and origin == 'SFO' and destination == 'ORD'\")\n",
    ".orderBy(desc(\"delay\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------------+\n",
      "|delay|origin|destination|   Flight_Delays|\n",
      "+-----+------+-----------+----------------+\n",
      "|   92|   ABE|        ORD|    Short Delays|\n",
      "|   91|   ABE|        DTW|    Short Delays|\n",
      "|    9|   ABE|        ORD|Tolerable Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|    9|   ABE|        ATL|Tolerable Delays|\n",
      "|   89|   ABE|        DTW|    Short Delays|\n",
      "|   88|   ABE|        ATL|    Short Delays|\n",
      "|   86|   ABE|        DTW|    Short Delays|\n",
      "|   83|   ABE|        ORD|    Short Delays|\n",
      "|   80|   ABE|        DTW|    Short Delays|\n",
      "|    8|   ABE|        ATL|Tolerable Delays|\n",
      "|    8|   ABE|        DTW|Tolerable Delays|\n",
      "|    8|   ABE|        ATL|Tolerable Delays|\n",
      "|    8|   ABE|        DTW|Tolerable Delays|\n",
      "|   76|   ABE|        ATL|    Short Delays|\n",
      "|    7|   ABE|        ATL|Tolerable Delays|\n",
      "|    7|   ABE|        DTW|Tolerable Delays|\n",
      "+-----+------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "    CASE\n",
    "    WHEN delay > 360 THEN 'Very Long Delays'\n",
    "    WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "    WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "    WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "    WHEN delay = 0 THEN 'No Delays'\n",
    "    ELSE 'Early'\n",
    "    END AS Flight_Delays\n",
    "    FROM us_delay_flights_tbl\n",
    "    ORDER BY origin, delay DESC\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|    date|delay|distance|origin|destination|   Flight_Delays|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01020600|   -8|     369|   ABE|        DTW|           Early|\n",
      "|01021245|   -2|     602|   ABE|        ATL|           Early|\n",
      "|01020605|   -4|     602|   ABE|        ATL|           Early|\n",
      "|01031245|   -4|     602|   ABE|        ATL|           Early|\n",
      "|01030605|    0|     602|   ABE|        ATL|       No Delays|\n",
      "|01041243|   10|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01040605|   28|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01051245|   88|     602|   ABE|        ATL|    Short Delays|\n",
      "|01050605|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01061215|   -6|     602|   ABE|        ATL|           Early|\n",
      "|01061725|   69|     602|   ABE|        ATL|    Short Delays|\n",
      "|01061230|    0|     369|   ABE|        DTW|       No Delays|\n",
      "|01060625|   -3|     602|   ABE|        ATL|           Early|\n",
      "|01070600|    0|     369|   ABE|        DTW|       No Delays|\n",
      "|01071725|    0|     602|   ABE|        ATL|       No Delays|\n",
      "|01071230|    0|     369|   ABE|        DTW|       No Delays|\n",
      "|01070625|    0|     602|   ABE|        ATL|       No Delays|\n",
      "|01071219|    0|     569|   ABE|        ORD|       No Delays|\n",
      "|01080600|    0|     369|   ABE|        DTW|       No Delays|\n",
      "|01081230|   33|     369|   ABE|        DTW|Tolerable Delays|\n",
      "|01080625|    1|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01080607|    5|     569|   ABE|        ORD|Tolerable Delays|\n",
      "|01081219|   54|     569|   ABE|        ORD|Tolerable Delays|\n",
      "|01091215|   43|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01090600|  151|     369|   ABE|        DTW|     Long Delays|\n",
      "|01091725|    0|     602|   ABE|        ATL|       No Delays|\n",
      "|01091230|   -4|     369|   ABE|        DTW|           Early|\n",
      "|01090625|    8|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01091219|   83|     569|   ABE|        ORD|    Short Delays|\n",
      "|01101215|   -5|     602|   ABE|        ATL|           Early|\n",
      "|01100600|   -5|     369|   ABE|        DTW|           Early|\n",
      "|01101725|    7|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01101230|   -8|     369|   ABE|        DTW|           Early|\n",
      "|01100625|   52|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01101219|    0|     569|   ABE|        ORD|       No Delays|\n",
      "|01111215|  127|     602|   ABE|        ATL|     Long Delays|\n",
      "|01110600|   -9|     369|   ABE|        DTW|           Early|\n",
      "|01110625|   -4|     602|   ABE|        ATL|           Early|\n",
      "|01121215|   -5|     602|   ABE|        ATL|           Early|\n",
      "|01121725|   -1|     602|   ABE|        ATL|           Early|\n",
      "|01131215|   14|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01130600|   -7|     369|   ABE|        DTW|           Early|\n",
      "|01131725|   -6|     602|   ABE|        ATL|           Early|\n",
      "|01131230|  -13|     369|   ABE|        DTW|           Early|\n",
      "|01130625|   29|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01131219|   -8|     569|   ABE|        ORD|           Early|\n",
      "|01140600|   -9|     369|   ABE|        DTW|           Early|\n",
      "|01141725|   -9|     602|   ABE|        ATL|           Early|\n",
      "|01141230|   -8|     369|   ABE|        DTW|           Early|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"Flight_Delays\", when((col(\"delay\") > 360), \"Very Long Delays\")\n",
    "                                 .when((col(\"delay\") > 120) & (col(\"delay\") < 360), \"Long Delays\")\n",
    "                                 .when((col(\"delay\") > 60 ) & (col(\"delay\") < 120), \"Short Delays\")\n",
    "                                 .when((col(\"delay\") > 0) & (col(\"delay\") < 60), \"Tolerable Delays\")\n",
    "                                 .when((col(\"delay\") == 0), \"No Delays\")\n",
    "                                 .otherwise(lit('Early')))\n",
    "df2.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Namespace 'learn_spark_db' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29972\\3886658195.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# In Scala/Python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DROP DATABASE learn_spark_db CASCADE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CREATE  DATABASE learn_spark_db\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"USE learn_spark_db\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Namespace 'learn_spark_db' not found"
     ]
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.sql(\"DROP DATABASE learn_spark_db CASCADE\")\n",
    "spark.sql(\"CREATE  DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `learn_spark_db`.`managed_us_delay_flights_tbl2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24884\\1674027082.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CREATE TABLE learn_spark_db.managed_us_delay_flights_tbl2 (date STRING, delay INT,distance INT, origin STRING, destination STRING)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenzi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `learn_spark_db`.`managed_us_delay_flights_tbl2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE learn_spark_db.managed_us_delay_flights_tbl2 (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "#csv_file = \"same as before\"\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    "distance INT, origin STRING, destination STRING)\n",
    "USING csv OPTIONS (PATH\n",
    "'C:/Lenzi/Spark/spark developer preparation/data/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "origin = 'SFO' \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "origin = 'JFK'\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "|01011005|   -8|   SFO|        DFW|\n",
      "|01011800|    0|   SFO|        ORD|\n",
      "|01011740|   -7|   SFO|        LAX|\n",
      "|01012015|   -7|   SFO|        LAX|\n",
      "|01012110|   -1|   SFO|        MIA|\n",
      "|01011610|  134|   SFO|        DFW|\n",
      "|01011240|   -6|   SFO|        MIA|\n",
      "|01010755|   -3|   SFO|        DFW|\n",
      "|01010020|    0|   SFO|        DFW|\n",
      "|01010705|   -6|   SFO|        LAX|\n",
      "|01010925|   -3|   SFO|        ORD|\n",
      "|01010555|   -6|   SFO|        ORD|\n",
      "|01011105|   -8|   SFO|        DFW|\n",
      "|01012330|   32|   SFO|        ORD|\n",
      "|01011330|    3|   SFO|        DFW|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM us_origin_airport_JFK_tmp_view\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "|01011540|   -4|   JFK|        DFW|\n",
      "|01011705|    5|   JFK|        SAN|\n",
      "|01011530|   -3|   JFK|        SFO|\n",
      "|01011630|   -3|   JFK|        SJU|\n",
      "|01011345|    2|   JFK|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "|01011540|   -4|   JFK|        DFW|\n",
      "|01011705|    5|   JFK|        SAN|\n",
      "|01011530|   -3|   JFK|        SFO|\n",
      "|01011630|   -3|   JFK|        SJU|\n",
      "|01011345|    2|   JFK|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or\n",
    "spark.sql('''SELECT * FROM us_origin_airport_JFK_tmp_view''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view''')\n",
    "spark.sql('''DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Lenzi/Spark/spark%20developer%20preparation/notebooks/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Lenzi/Spark/spark%20developer%20preparation/notebooks/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Scala/Python\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- In SQL\n",
    "\n",
    "\n",
    "CACHE [LAZY] TABLE <table-name>\n",
    "UNCACHE TABLE <table-name>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Tables into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources for DataFrames and SQL Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_parquet = \"\"\"C:/Lenzi/Spark/spark developer preparation/data/flights/parquet/\"\"\"\n",
    "df = spark.sql('''select * from us_delay_flights_tbl''')\n",
    "df.write.parquet(file_parquet)\n",
    "df.write.json(\"\"\"C:/Lenzi/Spark/spark developer preparation/data/flights/json/\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python\n",
    "# Use Parquet\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(file_parquet)\n",
    "\n",
    "# Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "df2 = spark.read.load(file_parquet)\n",
    "\n",
    "# Use CSV\n",
    "csv_file = \"C:/Lenzi/Spark/spark developer preparation/data/flights/departuredelays.csv\"\n",
    "df3= (spark.read.format(\"csv\"))\n",
    "      \n",
    "# Use JSON\n",
    "df4 = spark.read.format(\"json\").load(\"\"\"C:/Lenzi/Spark/spark developer preparation/data/flights/json/*\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Use JSON\n",
    "location = (\"\"\"C:/Lenzi/Spark/spark developer preparation/data/flights/json/\"\"\")\n",
    "df.coalesce(1).write.format(\"json\").mode(\"overwrite\").save(location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Parquet files into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python\n",
    "file = \"\"\"C:/Lenzi/Spark/spark developer preparation/data/nyc-taxi/yellow_tripdata_2022-01.parquet\"\"\"\n",
    "dfnyctaxi = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFunc(df, rows):\n",
    "    df = df.pandas_api()\n",
    "    return df.head(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 01:35:40</td>\n",
       "      <td>2022-01-01 01:53:29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 01:33:43</td>\n",
       "      <td>2022-01-01 01:42:07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:53:21</td>\n",
       "      <td>2022-01-01 02:02:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:25:21</td>\n",
       "      <td>2022-01-01 01:35:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:36:48</td>\n",
       "      <td>2022-01-01 02:14:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
       "0         1  2022-01-01 01:35:40   2022-01-01 01:53:29              2.0           3.80         1.0                  N           142           236             1         14.5    3.0      0.5        3.65           0.0                    0.3         21.95                   2.5          0.0\n",
       "1         1  2022-01-01 01:33:43   2022-01-01 01:42:07              1.0           2.10         1.0                  N           236            42             1          8.0    0.5      0.5        4.00           0.0                    0.3         13.30                   0.0          0.0\n",
       "2         2  2022-01-01 01:53:21   2022-01-01 02:02:19              1.0           0.97         1.0                  N           166           166             1          7.5    0.5      0.5        1.76           0.0                    0.3         10.56                   0.0          0.0\n",
       "3         2  2022-01-01 01:25:21   2022-01-01 01:35:23              1.0           1.09         1.0                  N           114            68             2          8.0    0.5      0.5        0.00           0.0                    0.3         11.80                   2.5          0.0\n",
       "4         2  2022-01-01 01:36:48   2022-01-01 02:14:20              1.0           4.30         1.0                  N            68           163             1         23.5    0.5      0.5        3.00           0.0                    0.3         30.30                   2.5          0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displayFunc(dfnyctaxi,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Parquet files into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMPORARY VIEW nyc_taxi\n",
    "USING parquet\n",
    "OPTIONS (path \"C:/Lenzi/Spark/spark developer preparation/data/nyc-taxi/\") \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 01:35:40</td>\n",
       "      <td>2022-01-01 01:53:29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 01:33:43</td>\n",
       "      <td>2022-01-01 01:42:07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:53:21</td>\n",
       "      <td>2022-01-01 02:02:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:25:21</td>\n",
       "      <td>2022-01-01 01:35:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 01:36:48</td>\n",
       "      <td>2022-01-01 02:14:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
       "0         1  2022-01-01 01:35:40   2022-01-01 01:53:29              2.0           3.80         1.0                  N           142           236             1         14.5    3.0      0.5        3.65           0.0                    0.3         21.95                   2.5          0.0\n",
       "1         1  2022-01-01 01:33:43   2022-01-01 01:42:07              1.0           2.10         1.0                  N           236            42             1          8.0    0.5      0.5        4.00           0.0                    0.3         13.30                   0.0          0.0\n",
       "2         2  2022-01-01 01:53:21   2022-01-01 02:02:19              1.0           0.97         1.0                  N           166           166             1          7.5    0.5      0.5        1.76           0.0                    0.3         10.56                   0.0          0.0\n",
       "3         2  2022-01-01 01:25:21   2022-01-01 01:35:23              1.0           1.09         1.0                  N           114            68             2          8.0    0.5      0.5        0.00           0.0                    0.3         11.80                   2.5          0.0\n",
       "4         2  2022-01-01 01:36:48   2022-01-01 02:14:20              1.0           4.30         1.0                  N            68           163             1         23.5    0.5      0.5        3.00           0.0                    0.3         30.30                   2.5          0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In Python\n",
    "displayFunc(spark.sql(\"SELECT * FROM nyc_taxi\"),5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a JSON file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"C:/Lenzi/Spark/spark developer preparation/data/blogs.json\"\n",
    "dfjson = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a JSON file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMPORARY VIEW json_blog\n",
    "USING json\n",
    "OPTIONS (\n",
    "path \"C:/Lenzi/Spark/spark developer preparation/data/blogs.json\"\n",
    ")\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing DataFrames to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(dfjson.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Lenzi/Spark/spark developer preparation/data/blogs2.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20996747"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '''C:/Lenzi/Datasets/dados/estabelecimentos/estabelecimentos/'''\n",
    "dfcsv = spark.read.format(\"csv\").load(file)\n",
    "dfcsv.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a CSV file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMPORARY VIEW csv_estab\n",
    "USING json\n",
    "OPTIONS (\n",
    "path \"C:/Lenzi/Datasets/dados/estabelecimentos/estabelecimentos/\"\n",
    ")\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing DataFrames to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(dfcsv.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Lenzi/Datasets/dados/estabelecimentos/estabelecimentos2/\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an Avro file into a DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an Avro file into a Spark SQL table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing DataFrames to Avro files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an ORC file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20996747"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '''C:/Lenzi/Datasets/dados/estabelecimentos/ORC/'''\n",
    "dforc = spark.read.format(\"orc\").load(file)\n",
    "dforc.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an ORC file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMPORARY VIEW orc_estab\n",
    "USING orc\n",
    "OPTIONS (\n",
    "path \"C:/Lenzi/Datasets/dados/estabelecimentos/ORC/\"\n",
    ")\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing DataFrames to ORC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(dfcsv.write\n",
    ".format(\"ORC\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Lenzi/Datasets/dados/estabelecimentos/ORC/\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an image file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.ml import image\n",
    "image_dir = \"C:/Users/Lenzi/Desktop/GCM/penha.jpg\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+\n",
      "|height|width|nChannels|mode|\n",
      "+------+-----+---------+----+\n",
      "|896   |1200 |3        |16  |\n",
      "+------+-----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\").show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a binary file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/C:/Users/Le...|2023-01-26 14:00:...|194732|[FF D8 FF DB 00 4...|\n",
      "|file:/C:/Users/Le...|2023-01-26 13:57:...| 13790|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/Le...|2023-01-26 13:56:...| 12554|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/Le...|2023-01-26 14:04:...| 10856|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/Le...|2023-01-26 13:53:...|  9196|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "path = \"C:/Users/Lenzi/Desktop/GCM/*\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".load(path))\n",
    "binary_files_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ed1cb665153056a227012879eeccb1f3bdc4741eab748c2bfd24c235a4ad9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
