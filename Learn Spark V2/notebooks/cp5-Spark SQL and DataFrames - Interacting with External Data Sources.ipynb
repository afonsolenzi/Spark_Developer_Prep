{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference repos\n",
    "https://github.com/databricks/LearningSparkV2\n",
    "\n",
    "https://github.com/RodrigoLima82/spark-certification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"lean-spark-cap5\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc= SparkContext()\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import LongType\n",
    "# Create cubed function\n",
    "def power2(s):\n",
    "    return s * s\n",
    "# Register UDF\n",
    "spark.udf.register(\"power2\", power2, LongType())\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Scala/Python\n",
    "# Query the cubed UDF\n",
    "spark.sql(\"SELECT id, power2(id) AS id_power2 FROM udf_test\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation order and null checking in Spark SQL\n",
    "\n",
    "Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not\n",
    "guarantee the order of evaluation of subexpressions\n",
    "\n",
    "1. Make the UDF itself null-aware and do null checking inside the UDF.\n",
    "2. Use IF or CASE WHEN expressions to do the null check and invoke the UDF in a\n",
    "conditional branch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speeding up and distributing PySpark UDFs with Pandas UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "# Create the pandas UDF for the cubed function\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Series\n",
    "x = pd.Series([1, 2, 3])\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.range(1, 4)\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying with the Spark SQL Shell, Beeline, and Tableau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Spark SQL Shell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the Spark SQL CLI, execute the following command in the $SPARK_HOME\n",
    "folder:\n",
    "\n",
    "./bin/spark-sql\n",
    "\n",
    "spark-sql> CREATE TABLE people (name STRING, age int);\n",
    "\n",
    "Insert data into the table\n",
    "\n",
    "INSERT INTO people SELECT name, age FROM ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Spark SQL query\n",
    "\n",
    "Now that you have data in your table, you can run Spark SQL queries against it. Let’s\n",
    "start by viewing what tables exist in our metastore:\n",
    "\n",
    "spark-sql> SHOW TABLES;\n",
    "\n",
    "default people false\n",
    "Time taken: 0.016 seconds, Fetched 1 row(s)\n",
    "\n",
    "Next, let’s find out how many people in our table are younger than 20 years of age:\n",
    "\n",
    "spark-sql> SELECT * FROM people WHERE age < 20;\n",
    "\n",
    "Samantha 19\n",
    "\n",
    "Time taken: 0.593 seconds, Fetched 1 row(s)\n",
    "\n",
    "As well, let’s see who the individuals are who did not specify their age:\n",
    "\n",
    "\n",
    "spark-sql> SELECT name FROM people WHERE age IS NULL;\n",
    "\n",
    "Michael\n",
    "Time taken: 0.272 seconds, Fetched 1 row(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Beeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Tableau\n",
    "\n",
    "./sbin/start-thriftserver.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Data Sources\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JDBC and SQL Databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"learn-spark-cap5\")\n",
    "    .config(\"spark.jars\", \"C:/Spark/jars/postgresql-42.5.4.jar\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Read Option 1: Loading data from a JDBC source using load method\n",
    "\n",
    "jdbcDF1 = (spark\n",
    "           .read\n",
    "           .format(\"jdbc\")\n",
    "           .option(\"url\", \"jdbc:postgresql://postgres\")\n",
    "           .option(\"dbtable\", \"public.aluno\")\n",
    "           .option(\"user\", \"master\")\n",
    "           .option(\"password\", \"388020\")\n",
    "           .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "jdbcDF2 = (spark\n",
    "           .read\n",
    "           .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "                 properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Option 1: Saving data to a JDBC source using save method\n",
    "(jdbcDF1\n",
    " .write\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    " .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    " .option(\"user\", \"[USERNAME]\")\n",
    " .option(\"password\", \"[PASSWORD]\")\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Option 2: Saving data to a JDBC source using jdbc method\n",
    "(jdbcDF2\n",
    " .write\n",
    " .jdbc(\"jdbc:postgresql:[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "       properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Loading data from a JDBC source using load\n",
    "jdbcDF = (spark\n",
    "          .read\n",
    "          .format(\"jdbc\")\n",
    "          .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "          .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "          .option(\"dbtable\", \"[TABLENAME]\")\n",
    "          .option(\"user\", \"[USERNAME]\")\n",
    "          .option(\"password\", \"[PASSWORD]\")\n",
    "          .load())\n",
    "# Saving data to a JDBC source using save\n",
    "(jdbcDF\n",
    " .write\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"[TABLENAME]\")\n",
    " .option(\"user\", \"[USERNAME]\")\n",
    " .option(\"password\", \"[PASSWORD]\")\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "###### SparkSession ######\n",
    "def session_spark():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"appDesafio\")\n",
    "            .config('spark.sql.debug.maxToStringFields', 500)\n",
    "            .config('spark.debug.maxToStringFields', 500)\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "            .config(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "            .config(\"spark.jars\", \"C:/Spark/jars/postgresql-42.5.4.jar\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "spark = session_spark()\n",
    "\n",
    "\n",
    "# set log level\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "## Tratamento de Shuffle Spark\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"jdbc\"\n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "dbtable = \"aluno\"\n",
    "username = \"postgres\"\n",
    "password = \"388020\"\n",
    "driver = \"org.postgresql.Driver\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Loading data from Azure Cosmos DB\n",
    "# Read configuration\n",
    "query = \"SELECT c.colA, c.coln FROM c WHERE c.origin = 'SEA'\"\n",
    "readConfig = {\n",
    "\"Endpoint\" : \"https://[ACCOUNT].documents.azure.com:443/\",\n",
    "\"Masterkey\" : \"[MASTER KEY]\",\n",
    "\"Database\" : \"[DATABASE]\",\n",
    "\"preferredRegions\" : \"Central US;East US2\",\"Collection\" : \"[COLLECTION]\",\n",
    "\"SamplingRatio\" : \"1.0\",\n",
    "\"schema_samplesize\" : \"1000\",\n",
    "\"query_pagesize\" : \"2147483647\",\n",
    "\"query_custom\" : query\n",
    "}\n",
    "# Connect via azure-cosmosdb-spark to create Spark DataFrame\n",
    "df = (spark\n",
    ".read\n",
    ".format(\"com.microsoft.azure.cosmosdb.spark\")\n",
    ".options(**readConfig)\n",
    ".load())\n",
    "# Count the number of flights\n",
    "df.count()\n",
    "# Saving data to Azure Cosmos DB\n",
    "# Write configuration\n",
    "writeConfig = {\n",
    "\"Endpoint\" : \"https://[ACCOUNT].documents.azure.com:443/\",\n",
    "\"Masterkey\" : \"[MASTER KEY]\",\n",
    "\"Database\" : \"[DATABASE]\",\n",
    "\"Collection\" : \"[COLLECTION]\",\n",
    "\"Upsert\" : \"true\"\n",
    "}\n",
    "# Upsert the DataFrame to Azure Cosmos DB\n",
    "(df.write\n",
    ".format(\"com.microsoft.azure.cosmosdb.spark\")\n",
    ".options(**writeConfig)\n",
    ".save())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Configure jdbcUrl\n",
    "jdbcUrl = \"jdbc:sqlserver://[DBSERVER]:1433;database=[DATABASE]\"\n",
    "# Loading data from a JDBC source\n",
    "jdbcDF = (spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", jdbcUrl)\n",
    ".option(\"dbtable\", \"[TABLENAME]\")\n",
    ".option(\"user\", \"[USERNAME]\")\n",
    ".option(\"password\", \"[PASSWORD]\")\n",
    ".load())\n",
    "# Saving data to a JDBC source\n",
    "(jdbcDF\n",
    ".write\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", jdbcUrl)\n",
    ".option(\"dbtable\", \"[TABLENAME]\")\n",
    ".option(\"user\", \"[USERNAME]\")\n",
    ".option(\"password\", \"[PASSWORD]\")\n",
    ".save())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other External Sources\n",
    "\n",
    "There are just some of the many external data sources Apache Spark can connect to;\n",
    "other popular data sources include:\n",
    "\n",
    "• Apache Cassandra\n",
    "\n",
    "• Snowflake\n",
    "\n",
    "• MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Explode and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- In SQL\n",
    "from pyspark.sql.functions import explode,collect_list\n",
    "\n",
    "spark.sql('''\n",
    "SELECT id, collect_list(value + 1) AS values\n",
    "FROM (SELECT id, EXPLODE(values) AS value\n",
    "FROM dfFromData2) x\n",
    "GROUP BY id'''\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While collect_list() returns a list of objects with duplicates, the GROUP BY statement\n",
    "requires shuffle operations, meaning the order of the re-collected array isn’t\n",
    "necessarily the same as that of the original array. As values could be any number of\n",
    "dimensions (a really wide and/or really long array) and we’re doing a GROUP BY, this\n",
    "approach could be very expensive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: User-Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- In SQL\n",
    "spark.sql('''\n",
    "SELECT id, collect_list(value + 1) AS values\n",
    "FROM (SELECT id, EXPLODE(values) AS value\n",
    "FROM table) x\n",
    "GROUP BY id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_distinct(array(1,2, 3, null, 3));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_intersect(array(1, 2, 3), array(1,3, 5));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_union(array(1, 2,3), array(1, 3, 5));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_except(array(1,2, 3), array(1, 3, 5));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_join(array('hello','world'), ' ');''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_max(array(1, 20,null, 3));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_min(array(1, 20,null, 3));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_position(array(3,2, 1), 1);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_remove(array(1,2, 3, null, 3), 3);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT arrays_overlap(array(1,2, 3), array(3, 4, 5));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_sort(array('b','d', null, 'c', 'a'));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT concat(array(1, 2, 3),array(4, 5), array(6));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT flatten(array(array(1,2), array(3, 4)));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT array_repeat('123', 3);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT reverse(array(2, 1, 4,3));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT sequence(to_date('2018-01-01'),to_date('2018-05-01'), interval 1 month);''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT shuffle(array(1, 20,null, 3));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT slice(array(1, 2, 3,4), -3, 3);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT arrays_zip(array(1, 2),array(2, 3), array(3, 4));''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT element_at(array(1, 2,3), 2);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT cardinality(array('x','b','d', 'c', 'a'));''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT map_from_arrays(array(1.0,3.0,5.0), array('2', '4','5'));''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT map_from_entries(array(struct(1,'a'), struct(2, 'b')));''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT element_at(map(1, 'a',2, 'b'), 2);''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT cardinality(map(1, 'a',2, 'b',3,'z'));''').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGH ORDER FUNCTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "# Show the DataFrame\n",
    "t_c.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Scala/Python\n",
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "filter(celsius, t -> t > 38) as high\n",
    "FROM tC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "reduce(\n",
    "celsius,\n",
    "0,\n",
    "(t, acc) -> t + acc,\n",
    "acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    ") as avgFahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrames and Spark SQL Operations\n",
    "\n",
    "Part of the power of Spark SQL comes from the wide range of DataFrame operations\n",
    "(also known as untyped Dataset operations) it supports. The list of operations is quite\n",
    "extensive and includes:\n",
    "• Aggregate functions\n",
    "• Collection functions\n",
    "• Datetime functions\n",
    "• Math functions\n",
    "• Miscellaneous functions\n",
    "• Non-aggregate functions\n",
    "• Sorting functions\n",
    "• String functions\n",
    "• UDF functions\n",
    "• Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath = \"C:/Lenzi/Spark/spark developer preparation/data/flights/departuredelays.csv\"\n",
    "airportsnaFilePath = \"C:/Lenzi/Spark/spark developer preparation/data/flights/airport-codes-na.txt\"\n",
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    ".format(\"csv\")\n",
    ".options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    ".load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    ".format(\"csv\")\n",
    ".options(header=\"true\")\n",
    ".load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    ".withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    ".withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    ".filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
    "date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView(\"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have duplicates\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bar\n",
    "WHERE origin = 'SEA'\n",
    "AND destination = 'SFO'\n",
    "AND date LIKE '01010%'\n",
    "AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.join(\n",
    "airportsna,\n",
    "airportsna.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "FROM foo f\n",
    "JOIN airports_na a\n",
    "ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "DROP TABLE IF EXISTS departureDelaysWindow;\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalogImplementation\", \"hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=spark.sql('''\n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    "FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    "AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination; \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"departureDelaysWindow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT * FROM departureDelaysWindow''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT origin, destination, SUM(TotalDelays) AS TotalDelays\n",
    "FROM departureDelaysWindow\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    "GROUP BY origin, destination\n",
    "ORDER BY SUM(TotalDelays) DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "+------+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank\n",
    "FROM (\n",
    "SELECT origin, destination, TotalDelays, dense_rank()\n",
    "OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "FROM departureDelaysWindow\n",
    ") t\n",
    "WHERE rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    "\"status\",\n",
    "expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "))\n",
    "foo2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------------+\n",
      "|    date|delay|distance|origin|destination|flight_status|\n",
      "+--------+-----+--------+------+-----------+-------------+\n",
      "|01010710|   31|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|      On-time|\n",
      "+--------+-----+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "foo4 = foo2.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-- In SQL\n",
    "spark.sql('''SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "''').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x00000292371E5760>>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ed1cb665153056a227012879eeccb1f3bdc4741eab748c2bfd24c235a4ad9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
